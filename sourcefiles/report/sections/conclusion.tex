\section{Conclusion}

A simple, general architecture with six variations was investigated in the report with the purpose of fine-tuning models for fine-grained highly domain-specific text-based zero-shot image retrieval. The experiments show that there is some linearly expressible similarity between the embedding spaces of text and image classifiers fine-tuned on different but semantically similar tasks. These similarities provide us with insights about the compatibility of different architectures. DistilBERT was found to be of higher utility than TF-IDF in this context. Surprisingly, it would be difficult to name a single image classifier network as the winner. Even the eldest, VGG, have managed to outperform the rest in certain cases. 

Even though all six networks are outperformed by the setup of Reed et al. \cite{reed2016learning}, they still perform surprisingly well given their generic late-fusion architecture. The experiments reveal an interesting potential: combining off-the-shelf networks for highly domain-specific problems is a viable solution. It might not result in state-of-the-art performance --- for that, a custom architecture is likely necessary --- but they could serve as a simple baseline for new problems.
